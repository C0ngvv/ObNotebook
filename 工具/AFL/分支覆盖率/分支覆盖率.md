## 1.覆盖率检测
向目标程序注入以下工具来捕获分支(边缘)覆盖率和分支命中计数
- 一条边表示为当前基本块(分配了一个随机常数)与前一个基本块(即元组 (prev_location, cur_location))之间的 XOR
- 覆盖信息存储在一个紧凑共享的64KB的哈希表中，称为跟踪位图(bitmap)
- 当访问一条边时，通过增加与该特定边的哈希相对应的位图中的值来记录命中

```
cur_location = <COMPILE_TIME_RANDOM>; # cur_location 值是随机生成的，以简化链接复杂项目的过程并保持 XOR 输出均匀分布。
shared_mem[cur_location ^ prev_location]++; # 由调用者传递给检测的二进制文件。
prev_location = cur_location >> 1; # 保持元组的方向性 以区分 A^B 和 B^A ,并保持紧密循环的标识,区分A^A,B^B
```

## 2.新路径检测
### 1.检测新元组
### 2.桶（元组命中数）
```
1, 2, 3, 4-7, 8-15, 16-31, 32-127, 128+
```

buckets 的数量是一种实现工具(implementation artifact)，它可以将由8-bit计数器到8-position位图的映射，前者由插桩产生，后者由 AFL 运行时路径已出现的元组数来计数。

在单个桶范围内的变化会被忽略； 从一个桶到另一个桶的转换被标记为程序控制流中有趣的变化。也会指导下一阶段的进化过程 evolutionary process

### 3.输入队列进化
产生了新的程序状态转换的变异测试用例会被添加到输入队列中，并用作新一轮次 fuzzing 的起点。它们作为已有测试用例的补充，但并不替换掉已有测试用例。

### 4. 精简语料库(Culling the corpus)
在进行fuzzing测试时，后期生成的某些测试用例可能会具有比其祖先提供的覆盖范围更为严格的边缘覆盖。换句话说，这种方法可以帮助测试人员发现更多的程序漏洞，因为后期生成的测试用例具有更高的覆盖率。

为了优化fuzzing测试，AFL定期使用快速算法重新评估队列：选择一小部分测试用例，这些测试用例仍然覆盖到目前为止见到的每个元组，并且它们的特征使它们对工具特别有利。

在这个过程中，AFL会生成一个“喜爱”的输入文件集合，这些文件集合通常比起始数据集小5-10倍。非“喜爱”的输入文件不会被丢弃，但当它们在队列中遇到时，会以不同的概率被跳过：

- 如果队列中有新的、尚未进行fuzzing测试的“喜爱”文件，则99%的非“喜爱”文件将被跳过，以便处理“喜爱”文件。
- 如果队列中没有新的“喜爱”文件：
    - 如果当前的非“喜爱”文件已经进行过fuzzing测试，则95%的时间会跳过该文件。
    - 如果该文件还没有进行任何fuzzing测试，则跳过的概率下降到75%。

### 5.修剪输入文件
文件大小对fuzzing测试的性能有重大影响，因为大文件会使目标程序执行变慢，并降低变异操作所触及的重要格式控制结构的可能性，从而降低测试的效率。

如果用户提供的初始输入文件(语料)质量较低，那么可能会导致AFL生成的测试用例也质量较低。而对于生成的文件大小不断增加的情况，这是因为某些变异策略会在每次迭代中添加更多的数据，从而导致生成的文件大小不断增加。

为了解决这个问题，AFL提供了一些性能优化技巧，如修剪输入文件和控制文件大小。
- 修剪输入文件是指删除不必要的部分，以减小文件大小；
- 控制文件大小是指限制每个测试用例的最大大小，以防止文件大小无限制地增加。

AFL在执行fuzzing测试时，通过对程序进行插桩（instrumentation）来获取有关程序执行路径的反馈信息。利用这些反馈信息，AFL可以自动识别输入文件中不必要的部分，并将其删除，从而减小文件大小。同时，由于这些更改不会影响程序的执行路径，因此可以确保测试用例的质量不会受到影响。

AFL中内置的修剪器(trimmer)会尝试按顺序删除具有可变长度和跨步的数据块；任何不影响路径映射校验和的删除操作都将被提交到磁盘。修剪器并不旨在彻底地修剪输入文件；相反，它试图在精度和进程上的execve()调用数量之间取得平衡，选择匹配块大小和跨步的值。每个文件的平均增益约为5-20%。

### 6.模糊测试策略
AFL通过一系列逐渐复杂但详尽和确定性的fuzzing策略（例如顺序位翻转和简单算术）来接近每个新输入文件，然后再进入纯随机行为。这样做的原因是先生成最简单和最优雅的测试用例；但是，该设计还提供了一种非常好的方法来量化每种新策略带来的价值 - 以及我们是否需要它。

特别是在早期阶段，afl-fuzz大部分工作都是高度确定性的，只有在后期才会逐渐转向随机堆叠修改和测试用例拼接。这些确定性策略包括：

- 顺序位翻转，步长与翻转长度可变
- 对小整数进行顺序加减
- 顺序插入特殊整数（0、1、`INT_MAX`等）

这些策略往往会产生紧凑的测试用例，并且非崩溃和崩溃输入之间的差异较小。

在确定性fuzzing完成后，AFL会使用非确定性策略，包括堆叠位翻转、插入、删除、算术运算和不同测试用例的拼接。

AFL通常不会去考虑特定变异和程序状态之间的关系，主要是出于性能、简单性和可靠性的考虑。fuzzing步骤通常是盲目(随机)的，只由输入队列的进化设计来指导。

### 7.字典构建
作者提出了一个简单的算法，该算法可以通过检测连续的比特翻转触发与邻近区域不同但整个字节序列中一致的执行路径来识别语法单元，并将其添加到字典中以供后续随机组合。这种方法虽然不能替代手工构建的关键字列表，但可以帮助那些没有时间或倾向于构建完整字典的人们。

AFL通过将基本的、容易获取的语法token以纯随机方式组合在一起，利用插桩技术和进化设计队列提供反馈机制，以区分无意义的变异和触发仪器代码中新行为的变异，并逐步在这个发现的基础上构建更复杂的语法。

有趣的是，AFL的插桩技术还允许fuzzer自动隔离已经存在于输入文件中的语法token。它在通过在翻转比特时定位对程序执行路径产生一致性改变的位置来实现；这表明代码中内置了与预定义值的原子比较。fuzzer依赖于这个信号来构建紧凑的”自动字典”，然后与其他fuzzing策略一起使用。

### 8. 崩溃去重
afl-fuzz实现的解决方案认为，如果满足以下两个条件之一，崩溃就是唯一的：
- 崩溃跟踪包含以前未见过的元组。
- 崩溃跟踪缺少先前所有故障中始终存在的元组。

### 9. 崩溃调查
许多类型的崩溃漏洞的可利用性是模糊不清的；afl-fuzz 试图通过提供一个崩溃探索模式(crash exploration mode)来解决这个问题，在这种模式下，一个已知存在漏洞的测试用例会被以与fuzzer正常操作非常相似的方式进行fuzzing，但有一个约束条件，即任何非崩溃的变异都将被丢弃。

对于崩溃来说，与通常的队列元素相反，将崩溃的输入进行减枝是毫无意义的。它们被发现之后就会原封不动的保存，以便将它们与父（未崩溃）元素进行对比分析。

### 10. fork服务器
为了提高性能，afl-fuzz使用了一个”fork server”，其中fuzzing过程中只需经过一次execve()、链接和libc初始化，然后通过利用写时复制(copy-on-write)从停止的进程映像中复制、clone。







## 参考

[【译】AFL白皮书 | 郁涛丶's Blog (ghostasky.github.io)](https://ghostasky.github.io/2023/05/16/2023-5AFLWritePaper/)

[AFL(american fuzzy lop)学习二_afl 中粧点边计算-CSDN博客](https://blog.csdn.net/sizaif/article/details/124268192)

